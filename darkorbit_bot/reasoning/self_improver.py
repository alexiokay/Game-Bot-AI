"""
DarkOrbit Bot - Self-Improvement System

Watches the bot play and uses VLM to critique its actions.
Creates correction data that can be used to improve training.

Two modes:
1. WATCH_BOT: VLM critiques bot actions, generates corrections
2. WATCH_PLAYER: VLM learns from human gameplay (existing system)

The key insight: VLM can tell us WHAT the bot SHOULD do,
not just whether what it did was good/bad.

Usage:
    # Run alongside the bot
    python self_improver.py --mode bot

    # Or analyze saved bot sessions
    python self_improver.py --session latest
"""

import json
import time
import sys
import base64
import threading
from pathlib import Path
from typing import Optional, Dict, List
from dataclasses import dataclass, asdict
from datetime import datetime
from io import BytesIO

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import requests
except ImportError:
    requests = None

try:
    from PIL import Image
    import numpy as np
except ImportError:
    Image = None
    np = None


@dataclass
class Correction:
    """A correction generated by VLM critique."""
    timestamp: float
    screenshot_path: str

    # What was happening
    situation: str  # combat/looting/exploring/idle
    threat_level: str

    # What the bot did
    bot_action: Dict  # {move_x, move_y, clicked, mode}

    # What it SHOULD have done (from VLM)
    correct_action: Dict  # {move_x, move_y, should_click, target_type}

    # Quality assessment
    quality: str  # good/needs_improvement/bad
    reasoning: str

    # For training
    state_vector: Optional[List[float]] = None


class SelfImprover:
    """
    Watches bot gameplay and generates corrections using VLM.

    The VLM doesn't just say "bad" - it tells us what the correct action
    should be, which we can use as training data.
    """

    # Path to system prompt file - EDIT THIS FILE to customize VLM behavior!
    # Location: darkorbit_bot/data/vlm_system_prompt.txt
    SYSTEM_PROMPT_FILE = Path(__file__).parent.parent / "data" / "vlm_system_prompt.txt"

    # Default fallback (used only if file not found)
    DEFAULT_SYSTEM_PROMPT = "You are an expert DarkOrbit game analyst helping improve an AI bot."

    # Cached system prompt (loaded once)
    _cached_system_prompt = None

    @classmethod
    def get_system_prompt(cls) -> str:
        """
        Load system prompt from file.

        Edit darkorbit_bot/data/vlm_system_prompt.txt to customize:
        - Enemy knowledge (threat levels, distances)
        - Combat tactics (orbiting, kiting, etc.)
        - Control mappings
        - Good/bad behavior definitions
        """
        # Return cached version if available
        if cls._cached_system_prompt is not None:
            return cls._cached_system_prompt

        try:
            if cls.SYSTEM_PROMPT_FILE.exists():
                cls._cached_system_prompt = cls.SYSTEM_PROMPT_FILE.read_text(encoding='utf-8')
                print(f"[SelfImprover] Loaded system prompt from: {cls.SYSTEM_PROMPT_FILE}")
            else:
                print(f"[SelfImprover] System prompt not found: {cls.SYSTEM_PROMPT_FILE}")
                print(f"[SelfImprover] Using default. Create this file to customize VLM!")
                cls._cached_system_prompt = cls.DEFAULT_SYSTEM_PROMPT
        except Exception as e:
            print(f"[SelfImprover] Error loading system prompt: {e}")
            cls._cached_system_prompt = cls.DEFAULT_SYSTEM_PROMPT

        return cls._cached_system_prompt

    @classmethod
    def reload_system_prompt(cls):
        """Force reload of system prompt from file (call after editing)."""
        cls._cached_system_prompt = None
        return cls.get_system_prompt()

    # MULTI-FRAME TEMPORAL CRITIQUE PROMPT
    # Analyzes a SEQUENCE of frames to understand bot behavior over time
    # This is much more powerful than single-frame analysis
    SEQUENCE_CRITIQUE_PROMPT = """You are analyzing a DarkOrbit game bot's behavior OVER TIME.

I'm showing you {num_frames} screenshots taken over the last {time_span:.1f} seconds.
The images are in chronological order: OLDEST first, NEWEST last.

RED BOXES = where YOLO detected enemies
YELLOW BOXES = where YOLO detected loot boxes
You must verify if these detections are REAL or FALSE POSITIVES.

=== TIMELINE OF BOT ACTIONS ===
{timeline}

=== EVENTS THAT OCCURRED ===
{events}

=== CURRENT STATE (latest frame) ===
- Mode: {mode}
- Mouse position: {bot_mouse}
- Clicked: {bot_clicked}
- Idle time: {idle_time}s
- Keyboard Actions: Ctrl(attack)={ctrl_attack}, Space(rocket)={space_rocket}, Shift(special)={shift_special}
- Is Attacking: {is_attacking}

YOUR ANALYSIS TASK:

1. VERIFY DETECTIONS: Look at the colored boxes in each frame.
   - Are the red boxes actually showing enemies?
   - Are yellow boxes actually showing loot?

2. EVALUATE THE SEQUENCE: Did the bot's actions make sense over time?
   - If enemy appeared: Did bot attack within reasonable time?
   - If enemy appeared: Is bot pressing Ctrl (attack key)? Is "Is Attacking" true?
   - If enemy present but NOT attacking: This is a CRITICAL BUG - bot should be attacking!
   - If enemy died: Did bot move to next target?
   - If no targets: Did bot explore to find new ones?
   - Was there wasted movement (going back and forth)?

3. DETECT COMBAT TACTICS (if in combat):
   Look at how the bot/player ship is moving relative to enemies:
   - ORBITING: Ship moves in circular pattern around enemy, maintaining distance
   - KITING: Ship moves away while attacking, keeps enemy at max range
   - FACE-TANKING: Ship stays stationary or moves directly at enemy
   - HIT-AND-RUN: Quick approach, attack, retreat pattern
   - DODGING: Erratic side-to-side movement to avoid projectiles
   - NONE: No clear tactical pattern (just clicking)

   Good players use ORBITING or KITING against tough enemies.
   Bad bots just click without movement tactics.

4. OUTCOME ASSESSMENT: What was the result?
   - Did the bot kill enemies? (GOOD)
   - Did the bot collect boxes? (GOOD)
   - Did the bot ignore opportunities? (BAD)
   - Did the bot click empty space? (BAD)

Reply ONLY with JSON:
{{
  "sequence_analysis": {{
    "real_enemies_seen": 0,
    "real_boxes_seen": 0,
    "false_positive_detections": 0,
    "enemies_attacked": 0,
    "boxes_collected": 0,
    "opportunities_missed": 0
  }},
  "combat_tactics": {{
    "detected_tactic": "orbiting/kiting/face_tanking/hit_and_run/dodging/none",
    "tactic_quality": "good/poor/none",
    "movement_pattern": "describe the ship's movement pattern relative to enemy",
    "recommended_tactic": "what tactic SHOULD be used for this enemy type?"
  }},
  "situation": "combat/looting/exploring/idle",
  "threat": "none/low/medium/high/critical",
  "bot_quality": "good/needs_improvement/bad",
  "reasoning": "explain the bot's behavior over the sequence - what did it do well or poorly?",
  "main_issue": "what was the biggest problem (if any)?",
  "correct_action": {{
    "move_x": 0.5,
    "move_y": 0.5,
    "should_click": true,
    "target_type": "enemy/box/explore/none",
    "priority": "what should bot focus on NOW based on what you see?",
    "tactic": "orbiting/kiting/face_tanking/none"
  }}
}}"""

    # Fallback single-frame prompt (when not enough frames)
    SINGLE_FRAME_PROMPT = """You are analyzing a DarkOrbit game bot's behavior.

RED BOXES = YOLO-detected enemies (verify if real!)
YELLOW BOXES = YOLO-detected loot boxes (verify if real!)

BOT STATE:
- Mode: {mode}
- Mouse position: {bot_mouse}
- Clicked: {bot_clicked}
- Idle time: {idle_time}s
- Detections: {num_enemies} enemies, {num_boxes} boxes claimed
- Keyboard Actions: Ctrl(attack)={ctrl_attack}, Space(rocket)={space_rocket}, Shift(special)={shift_special}
- Is Attacking: {is_attacking}

RECENT ACTIONS:
{action_history}

EVENTS:
{recent_events}

TASK: Verify the colored boxes contain REAL targets, then evaluate if bot action was correct.
IMPORTANT: If enemies are present, the bot MUST be attacking (Ctrl=True, Is Attacking=True).
If you see enemies but bot is NOT attacking, rate as BAD - this is a critical bug!

Reply ONLY with JSON:
{{
  "actually_visible": {{
    "real_enemies": 0,
    "real_boxes": 0,
    "false_positives": 0
  }},
  "situation": "combat/looting/exploring/idle",
  "threat": "none/low/medium/high/critical",
  "bot_quality": "good/needs_improvement/bad",
  "reasoning": "brief explanation",
  "correct_action": {{
    "move_x": 0.5,
    "move_y": 0.5,
    "should_click": true,
    "target_type": "enemy/box/explore/none"
  }}
}}"""

    def __init__(self,
                 model: str = "qwen/qwen3-vl-8b",
                 base_url: str = "http://localhost:1234",
                 critique_interval: float = 3.0):
        """
        Initialize self-improver.

        Args:
            model: VLM model name
            base_url: LM Studio API URL
            critique_interval: Seconds between critiques (VLM is slow)
        """
        self.model = model
        self.base_url = base_url
        self.critique_interval = critique_interval
        self.available = requests is not None and Image is not None

        if not self.available:
            print("Warning: 'requests' or 'PIL' not installed")

        # Corrections storage
        self.corrections: List[Correction] = []
        self.session_dir: Optional[Path] = None

        # Stats
        self.total_critiques = 0
        self.good_count = 0
        self.needs_improvement_count = 0
        self.bad_count = 0

        # Threading
        self.running = False
        self.critique_thread: Optional[threading.Thread] = None
        self.pending_frame = None
        self.pending_context = None
        self.lock = threading.Lock()

        # Temporal context - track recent actions and events
        self.action_history: List[Dict] = []  # Last N actions
        self.recent_events: List[str] = []    # Recent kills, pickups, etc.
        self.max_history = 10  # Keep last 10 actions (increased for better context)
        self.max_events = 10   # Keep last 10 events

        # MULTI-FRAME BUFFER for temporal VLM critique
        # Instead of single frame, we keep a sliding window of recent frames
        self.frame_buffer: List[Dict] = []  # [{frame, context, timestamp}, ...]
        self.max_buffer_frames = 5  # Keep last 5 frames (covering ~15s at 3s interval)
        self.frame_capture_interval = 3.0  # Capture frame every 3 seconds
        self.last_frame_capture = 0

    def start_session(self):
        """Start a new critique session."""
        self.session_dir = Path("data/corrections") / f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.session_dir.mkdir(parents=True, exist_ok=True)
        (self.session_dir / "screenshots").mkdir(exist_ok=True)

        self.corrections = []
        self.total_critiques = 0
        self.good_count = 0
        self.needs_improvement_count = 0
        self.bad_count = 0

        print(f"\n[SelfImprover] Session started: {self.session_dir.name}")

    def start_watching(self):
        """Start background critique thread."""
        self.running = True
        self.critique_thread = threading.Thread(target=self._critique_loop, daemon=True)
        self.critique_thread.start()
        print("[SelfImprover] Watching bot actions...")

    def stop_watching(self):
        """Stop watching and save corrections."""
        self.running = False
        if self.critique_thread:
            self.critique_thread.join(timeout=2.0)
        self._save_corrections()
        self._print_stats()

    def record_action(self, action: Dict, mode: str):
        """
        Record an action to the history buffer.
        Call this every frame to build temporal context.

        Args:
            action: Action dict with move_x, move_y, clicked, and keyboard actions
            mode: PASSIVE or AGGRESSIVE
        """
        entry = {
            'time': time.time(),
            'x': action.get('move_x', action.get('aim_x', 0.5)),
            'y': action.get('move_y', action.get('aim_y', 0.5)),
            'click': action.get('clicked', action.get('should_click', action.get('should_fire', False))),
            'mode': mode,
            # Keyboard actions
            'ctrl': action.get('ctrl_attack', False),
            'space': action.get('space_rocket', False),
            'shift': action.get('shift_special', False),
        }
        self.action_history.append(entry)
        # Keep only recent history
        if len(self.action_history) > self.max_history:
            self.action_history.pop(0)

    def record_event(self, event: str):
        """
        Record a game event (kill, pickup, damage, etc.).
        Call when something notable happens.

        Args:
            event: Description like "Killed Lordakia", "Picked up BonusBox"
        """
        timestamped = f"[{time.strftime('%H:%M:%S')}] {event}"
        self.recent_events.append(timestamped)
        # Keep only recent events
        if len(self.recent_events) > self.max_events:
            self.recent_events.pop(0)

    def submit_frame(self, frame: np.ndarray, context: Dict):
        """
        Submit a frame for critique.

        This now builds a TEMPORAL BUFFER of frames for multi-frame VLM critique.
        Instead of critiquing single frames, we analyze sequences of bot behavior.

        Args:
            frame: Screenshot as numpy array
            context: Dict with bot_action, detections, mode, idle_time, state_vector
        """
        now = time.time()

        # Auto-record action to history
        if context and 'bot_action' in context:
            self.record_action(context['bot_action'], context.get('mode', 'PASSIVE'))

        # Add frame to temporal buffer (for multi-frame critique)
        if now - self.last_frame_capture >= self.frame_capture_interval:
            frame_data = {
                'frame': frame.copy() if frame is not None else None,
                'context': context.copy() if context else None,
                'timestamp': now,
                'action_history': list(self.action_history),  # Snapshot of history
                'recent_events': list(self.recent_events)     # Snapshot of events
            }
            self.frame_buffer.append(frame_data)
            if len(self.frame_buffer) > self.max_buffer_frames:
                self.frame_buffer.pop(0)
            self.last_frame_capture = now

        with self.lock:
            self.pending_frame = frame.copy() if frame is not None else None
            self.pending_context = context.copy() if context else None

    def _critique_loop(self):
        """Background loop that critiques bot actions using MULTI-FRAME analysis."""
        last_critique_time = 0

        while self.running:
            now = time.time()

            # Rate limit critiques - wait longer to accumulate frames
            # We want at least 3 frames in buffer for meaningful temporal analysis
            min_frames_for_critique = 3
            if now - last_critique_time < self.critique_interval * 2:  # 6 seconds between critiques
                time.sleep(0.1)
                continue

            # Check if we have enough frames for temporal analysis
            if len(self.frame_buffer) < min_frames_for_critique:
                time.sleep(0.1)
                continue

            # Get pending frame (latest) and the frame buffer
            frame = None
            context = None
            with self.lock:
                if self.pending_frame is not None:
                    frame = self.pending_frame
                    context = self.pending_context
                    self.pending_frame = None
                    self.pending_context = None

            if frame is None or context is None:
                time.sleep(0.1)
                continue

            # Use MULTI-FRAME critique for better temporal understanding
            correction, seq_stats = self._critique_sequence(frame, context, self.frame_buffer)
            if correction:
                self.corrections.append(correction)
                self._log_correction(correction, seq_stats)

            last_critique_time = now

    def _critique_sequence(self, current_frame: np.ndarray, current_context: Dict,
                           frame_buffer: List[Dict]) -> tuple:
        """
        Critique a SEQUENCE of frames for better temporal understanding.

        This sends multiple frames to the VLM so it can understand:
        - How the situation evolved over time
        - Whether the bot's sequence of actions was appropriate
        - If opportunities were missed or actions were delayed

        Args:
            current_frame: The latest frame
            current_context: Context for the latest frame
            frame_buffer: List of recent frames with their contexts

        Returns:
            Tuple of (Correction or None, sequence_stats dict or None)
        """
        try:
            if len(frame_buffer) < 2:
                # Fall back to single-frame critique
                correction = self._critique_frame(current_frame, current_context)
                return correction, None

            # Calculate time span
            oldest_time = frame_buffer[0]['timestamp']
            newest_time = frame_buffer[-1]['timestamp']
            time_span = newest_time - oldest_time

            # Build timeline of actions from frame buffer
            timeline_lines = []
            for i, frame_data in enumerate(frame_buffer):
                age = newest_time - frame_data['timestamp']
                ctx = frame_data.get('context', {})
                bot_action = ctx.get('bot_action', {})

                # Count detections
                detections = ctx.get('detections', [])
                enemies = len([d for d in detections if d.get('class_name') in
                              ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']])
                boxes = len([d for d in detections if d.get('class_name') == 'BonusBox'])

                # Keyboard actions
                keys_str = ""
                if bot_action.get('ctrl_attack'):
                    keys_str += " CTRL"
                if bot_action.get('space_rocket'):
                    keys_str += " SPACE"
                if bot_action.get('shift_special'):
                    keys_str += " SHIFT"

                timeline_lines.append(
                    f"  Frame {i+1} ({age:.1f}s ago): "
                    f"pos=({bot_action.get('move_x', 0.5):.2f}, {bot_action.get('move_y', 0.5):.2f}) "
                    f"click={bot_action.get('clicked', False)}{keys_str} "
                    f"enemies={enemies} boxes={boxes} "
                    f"mode={ctx.get('mode', 'PASSIVE')} "
                    f"attacking={ctx.get('is_attacking', False)}"
                )

            timeline_str = "\n".join(timeline_lines)

            # Collect all events from the sequence
            all_events = []
            for frame_data in frame_buffer:
                events = frame_data.get('recent_events', [])
                for e in events:
                    if e not in all_events:
                        all_events.append(e)
            events_str = "\n".join(f"  - {e}" for e in all_events[-10:]) if all_events else "  (no events)"

            # Create composite image with multiple frames
            # We'll create a horizontal strip of key frames
            images_b64 = self._create_sequence_image(frame_buffer, current_frame, current_context)
            if not images_b64:
                correction = self._critique_frame(current_frame, current_context)
                return correction, None

            # Build prompt with temporal context
            bot_action = current_context.get('bot_action', {})
            prompt = self.SEQUENCE_CRITIQUE_PROMPT.format(
                num_frames=len(frame_buffer) + 1,
                time_span=time_span,
                timeline=timeline_str,
                events=events_str,
                mode=current_context.get('mode', 'PASSIVE'),
                bot_mouse=f"({bot_action.get('move_x', 0.5):.2f}, {bot_action.get('move_y', 0.5):.2f})",
                bot_clicked=bot_action.get('clicked', False),
                idle_time=current_context.get('idle_time', 0),
                # Keyboard actions - critical for VLM to check if bot is attacking
                ctrl_attack=bot_action.get('ctrl_attack', False),
                space_rocket=bot_action.get('space_rocket', False),
                shift_special=bot_action.get('shift_special', False),
                is_attacking=current_context.get('is_attacking', False)
            )

            # Query VLM with the sequence image
            result = self._query_vlm(images_b64, prompt)
            if not result:
                return None, None

            # Save debug data
            self.total_critiques += 1
            ss_filename = f"critique_{self.total_critiques:04d}_sequence.jpg"
            ss_path = self.session_dir / "screenshots" / ss_filename

            # Save the sequence image for debugging
            self._save_sequence_debug(frame_buffer, current_frame, current_context, ss_path)

            # Save full debug data
            self._save_critique_debug(
                critique_id=self.total_critiques,
                prompt=prompt,
                detections=current_context.get('detections', []),
                bot_action=bot_action,
                context=current_context,
                vlm_result=result
            )

            # Build correction from result
            correct_action = result.get('correct_action', {})

            correction = Correction(
                timestamp=time.time(),
                screenshot_path=ss_filename,
                situation=result.get('situation', 'unknown'),
                threat_level=result.get('threat', 'unknown'),
                bot_action={
                    'move_x': bot_action.get('move_x', 0.5),
                    'move_y': bot_action.get('move_y', 0.5),
                    'clicked': bot_action.get('clicked', False),
                    'mode': current_context.get('mode', 'PASSIVE')
                },
                correct_action={
                    'move_x': correct_action.get('move_x', 0.5),
                    'move_y': correct_action.get('move_y', 0.5),
                    'should_click': correct_action.get('should_click', False),
                    'target_type': correct_action.get('target_type', 'none'),
                    'priority': correct_action.get('priority', 'none')
                },
                quality=result.get('bot_quality', 'unknown'),
                reasoning=result.get('reasoning', '') + f" | Main issue: {result.get('main_issue', 'none')}",
                state_vector=current_context.get('state_vector')
            )

            # Update stats
            quality = correction.quality.lower()
            if quality == 'good':
                self.good_count += 1
            elif quality == 'needs_improvement':
                self.needs_improvement_count += 1
            else:
                self.bad_count += 1

            # Extract sequence analysis stats for logging
            sequence_stats = result.get('sequence_analysis', {})
            return correction, sequence_stats

        except Exception as e:
            print(f"[SelfImprover] Sequence critique error: {e}")
            import traceback
            traceback.print_exc()
            return None, None

    def _create_sequence_image(self, frame_buffer: List[Dict], current_frame: np.ndarray,
                                current_context: Dict) -> Optional[str]:
        """
        Create a composite image showing multiple frames in a 2x2 grid.

        Layout:
        ┌─────────────┬─────────────┐
        │  Frame 1    │  Frame 2    │
        │    ↓        │    ↓        │
        ├─────────────┼─────────────┤
        │  Frame 3    │  CURRENT    │
        │             │    ★        │
        └─────────────┴─────────────┘

        This square layout gives VLM more pixels per frame to see bounding boxes.
        """
        try:
            frames_to_show = []

            # Add frames from buffer (with bounding boxes)
            for frame_data in frame_buffer[-3:]:  # Last 3 from buffer
                frame = frame_data.get('frame')
                ctx = frame_data.get('context', {})
                if frame is not None:
                    frames_to_show.append((frame, ctx.get('detections', [])))

            # Add current frame
            frames_to_show.append((current_frame, current_context.get('detections', [])))

            if not frames_to_show:
                return None

            # Pad to exactly 4 frames if needed (duplicate first frame)
            while len(frames_to_show) < 4:
                frames_to_show.insert(0, frames_to_show[0])

            # Only take last 4 frames
            frames_to_show = frames_to_show[-4:]

            # Create individual annotated images with larger frame numbers
            annotated_images = []
            from PIL import ImageDraw, ImageFont

            for i, (frame, detections) in enumerate(frames_to_show):
                img = self._annotate_frame(frame, detections)
                if img:
                    draw = ImageDraw.Draw(img)

                    # Determine label - last frame is CURRENT
                    is_current = (i == len(frames_to_show) - 1)
                    label = "★ CURRENT" if is_current else f"Frame {i+1}"

                    # Draw LARGE, visible frame number label
                    # Bigger background box for higher resolution
                    label_bg_width = 160 if is_current else 130
                    label_bg_height = 40
                    draw.rectangle([0, 0, label_bg_width, label_bg_height], fill=(0, 0, 0))

                    # Frame number text (larger font)
                    try:
                        font = ImageFont.truetype("arial.ttf", 28)
                    except:
                        font = ImageFont.load_default()

                    text_color = (255, 255, 0) if is_current else (255, 255, 255)
                    draw.text((10, 6), label, fill=text_color, font=font)

                    # Add LARGE arrow indicator for non-current frames
                    if not is_current:
                        img_w, img_h = img.size
                        # Draw arrow at bottom right pointing to sequence flow
                        arrow_x = img_w - 60
                        arrow_y = img_h - 50
                        # Draw arrow with background for visibility
                        draw.rectangle([arrow_x - 10, arrow_y - 5, arrow_x + 40, arrow_y + 35], fill=(0, 0, 0))
                        if i == 0 or i == 2:  # Right arrow (→)
                            draw.text((arrow_x, arrow_y), "→", fill=(255, 255, 0), font=font)
                        if i == 1:  # Down arrow (↓)
                            draw.text((arrow_x, arrow_y), "↓", fill=(255, 255, 0), font=font)

                    annotated_images.append(img)

            if len(annotated_images) < 4:
                return None

            # Create 2x2 grid composite with CORRECT 16:9 aspect ratio
            # Each frame at 640x360 (16:9) for proper aspect ratio - no squashing!
            # Total grid: 1280x720 - standard HD resolution
            frame_width = 640
            frame_height = 360  # 16:9 aspect ratio (640/360 = 1.777)
            grid_width = frame_width * 2  # 1280
            grid_height = frame_height * 2  # 720

            composite = Image.new('RGB', (grid_width, grid_height))

            # Place frames in 2x2 grid:
            # [0, 1]
            # [2, 3]
            positions = [
                (0, 0),                      # Frame 1 - top left
                (frame_width, 0),            # Frame 2 - top right
                (0, frame_height),           # Frame 3 - bottom left
                (frame_width, frame_height)  # CURRENT - bottom right
            ]

            for i, img in enumerate(annotated_images):
                # Resize while maintaining aspect ratio (letterbox/pillarbox if needed)
                img_resized = self._resize_maintain_aspect(img, frame_width, frame_height)
                composite.paste(img_resized, positions[i])

            # Draw grid lines for clarity
            draw = ImageDraw.Draw(composite)
            # Vertical center line
            draw.line([(frame_width, 0), (frame_width, grid_height)], fill=(100, 100, 100), width=3)
            # Horizontal center line
            draw.line([(0, frame_height), (grid_width, frame_height)], fill=(100, 100, 100), width=3)

            # Encode to base64
            buffer = BytesIO()
            composite.save(buffer, format='JPEG', quality=90)
            return base64.b64encode(buffer.getvalue()).decode('utf-8')

        except Exception as e:
            print(f"[SelfImprover] Error creating sequence image: {e}")
            return None

    def _resize_maintain_aspect(self, img: Image.Image, target_width: int, target_height: int) -> Image.Image:
        """
        Resize image to fit within target dimensions while maintaining aspect ratio.
        Adds black letterboxing/pillarboxing if needed to fill the exact target size.

        This prevents distortion of game screenshots (e.g., 16:9 squeezed to 4:3).
        """
        orig_width, orig_height = img.size
        orig_aspect = orig_width / orig_height
        target_aspect = target_width / target_height

        if orig_aspect > target_aspect:
            # Image is wider - fit to width, letterbox top/bottom
            new_width = target_width
            new_height = int(target_width / orig_aspect)
        else:
            # Image is taller - fit to height, pillarbox left/right
            new_height = target_height
            new_width = int(target_height * orig_aspect)

        # Resize maintaining aspect ratio
        img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

        # Create black background at exact target size
        result = Image.new('RGB', (target_width, target_height), (0, 0, 0))

        # Center the resized image
        paste_x = (target_width - new_width) // 2
        paste_y = (target_height - new_height) // 2
        result.paste(img_resized, (paste_x, paste_y))

        return result

    def _annotate_frame(self, frame: np.ndarray, detections: List) -> Optional[Image.Image]:
        """Annotate a single frame with bounding boxes."""
        try:
            if frame is None:
                return None

            if frame.dtype != np.uint8:
                frame = (frame * 255).astype(np.uint8)

            # Convert BGR to RGB
            import cv2
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)

            # Draw bounding boxes
            if detections:
                from PIL import ImageDraw
                draw = ImageDraw.Draw(img)
                enemy_classes = ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']

                for d in detections:
                    class_name = d.get('class_name', 'unknown')
                    x_center = d.get('x_center', 0.5)
                    y_center = d.get('y_center', 0.5)

                    img_w, img_h = img.size
                    cx = int(x_center * img_w)
                    cy = int(y_center * img_h)
                    # Larger boxes (5% of image) for better visibility
                    box_half = max(40, int(min(img_w, img_h) * 0.05))

                    if class_name in enemy_classes:
                        color = (255, 0, 0)  # Red for enemies
                    elif class_name == 'BonusBox':
                        color = (255, 255, 0)  # Yellow for boxes
                    else:
                        color = (0, 255, 0)  # Green for other

                    # THICK boxes (width=6) for VLM to easily see
                    draw.rectangle(
                        [cx - box_half, cy - box_half, cx + box_half, cy + box_half],
                        outline=color, width=6
                    )
                    # Larger class label above box (bigger font area)
                    label_text = class_name[:8]  # Allow longer names
                    label_height = 24
                    label_width = len(label_text) * 10 + 8
                    draw.rectangle(
                        [cx - box_half, cy - box_half - label_height,
                         cx - box_half + label_width, cy - box_half],
                        fill=color
                    )
                    draw.text((cx - box_half + 4, cy - box_half - label_height + 4),
                              label_text, fill=(0, 0, 0))

            return img

        except Exception:
            return None

    def _save_sequence_debug(self, frame_buffer: List[Dict], current_frame: np.ndarray,
                              current_context: Dict, path: Path):
        """Save the sequence composite image for debugging."""
        try:
            img_b64 = self._create_sequence_image(frame_buffer, current_frame, current_context)
            if img_b64:
                img_data = base64.b64decode(img_b64)
                with open(path, 'wb') as f:
                    f.write(img_data)
        except Exception as e:
            print(f"[SelfImprover] Error saving sequence debug: {e}")

    def _critique_frame(self, frame: np.ndarray, context: Dict) -> Optional[Correction]:
        """Critique a single frame using VLM (fallback for when buffer is empty)."""
        try:
            # Build prompt
            bot_action = context.get('bot_action', {})
            detections = context.get('detections', [])

            # Encode image WITH detection boxes drawn
            # This helps VLM see exactly where YOLO thinks enemies are
            image_b64 = self._encode_image(frame, detections)
            if not image_b64:
                return None

            # NOTE: We no longer skip VLM critique even when bot seems "on target"
            # because YOLO detections may be FALSE POSITIVES. We need VLM to verify
            # what's actually visible in the screenshot.

            # Extract detections (may be wrong - VLM will verify)
            enemies = [d for d in detections if d.get('class_name') in
                      ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']]
            boxes = [d for d in detections if d.get('class_name') == 'BonusBox']

            # Format action history for temporal context
            if self.action_history:
                history_lines = []
                for i, act in enumerate(reversed(self.action_history)):
                    age = time.time() - act['time']
                    # Include keyboard actions in history
                    keys_str = ""
                    if act.get('ctrl'):
                        keys_str += " CTRL"
                    if act.get('space'):
                        keys_str += " SPACE"
                    if act.get('shift'):
                        keys_str += " SHIFT"
                    history_lines.append(
                        f"  {i+1}. ({act['x']:.2f}, {act['y']:.2f}) click={act['click']}{keys_str} [{act['mode']}] ({age:.1f}s ago)"
                    )
                action_history_str = "\n".join(history_lines)
            else:
                action_history_str = "  (no history yet - bot just started)"

            # Format recent events
            if self.recent_events:
                events_str = "\n".join(f"  - {e}" for e in self.recent_events)
            else:
                events_str = "  (no events recorded yet)"

            prompt = self.SINGLE_FRAME_PROMPT.format(
                mode=context.get('mode', 'PASSIVE'),
                bot_mouse=f"({bot_action.get('move_x', 0.5):.2f}, {bot_action.get('move_y', 0.5):.2f})",
                bot_clicked=bot_action.get('clicked', False),
                num_enemies=len(enemies),
                num_boxes=len(boxes),
                idle_time=context.get('idle_time', 0),
                action_history=action_history_str,
                recent_events=events_str,
                # Keyboard actions - critical for VLM to check if bot is attacking
                ctrl_attack=bot_action.get('ctrl_attack', False),
                space_rocket=bot_action.get('space_rocket', False),
                shift_special=bot_action.get('shift_special', False),
                is_attacking=context.get('is_attacking', False)
            )

            # Query VLM
            result = self._query_vlm(image_b64, prompt)
            if not result:
                return None

            # Save ANNOTATED screenshot (with bounding boxes) for debugging
            ss_filename = f"critique_{self.total_critiques:04d}.jpg"
            ss_path = self.session_dir / "screenshots" / ss_filename
            self._save_annotated_screenshot(frame, detections, ss_path)

            # Save full debug data (prompt, context, result) for review
            self._save_critique_debug(
                critique_id=self.total_critiques,
                prompt=prompt,
                detections=detections,
                bot_action=bot_action,
                context=context,
                vlm_result=result
            )

            # Build correction
            correct_action = result.get('correct_action', {})

            correction = Correction(
                timestamp=time.time(),
                screenshot_path=ss_filename,
                situation=result.get('situation', 'unknown'),
                threat_level=result.get('threat', 'unknown'),
                bot_action={
                    'move_x': bot_action.get('move_x', 0.5),
                    'move_y': bot_action.get('move_y', 0.5),
                    'clicked': bot_action.get('clicked', False),
                    'mode': context.get('mode', 'PASSIVE')
                },
                correct_action={
                    'move_x': correct_action.get('move_x', 0.5),
                    'move_y': correct_action.get('move_y', 0.5),
                    'should_click': correct_action.get('should_click', False),
                    'target_type': correct_action.get('target_type', 'none'),
                    'target_name': correct_action.get('target_name', 'none')
                },
                quality=result.get('bot_quality', 'unknown'),
                reasoning=result.get('reasoning', ''),
                state_vector=context.get('state_vector')
            )

            # Update stats
            self.total_critiques += 1
            if correction.quality == 'good':
                self.good_count += 1
            elif correction.quality == 'needs_improvement':
                self.needs_improvement_count += 1
            else:
                self.bad_count += 1

            return correction

        except Exception as e:
            print(f"[SelfImprover] Critique error: {e}")
            return None

    def _encode_image(self, frame: np.ndarray, detections: list = None) -> Optional[str]:
        """
        Encode numpy frame to base64, with optional detection boxes drawn.

        Drawing YOLO bounding boxes helps VLM see exactly where YOLO thinks
        enemies/boxes are, so it can verify if those areas actually contain
        valid targets or if YOLO is hallucinating.

        Args:
            frame: Numpy array (H, W, 3) - NOTE: OpenCV uses BGR, must convert to RGB!
            detections: List of detection dicts with class_name, x_center, y_center
        """
        try:
            if frame.dtype != np.uint8:
                frame = (frame * 255).astype(np.uint8)

            # CRITICAL: Convert BGR (OpenCV) to RGB (PIL) - fixes green tint!
            import cv2
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)

            # Draw detection boxes so VLM can see what YOLO detected
            if detections:
                try:
                    from PIL import ImageDraw, ImageFont
                    draw = ImageDraw.Draw(img)

                    # Colors: red for enemies, yellow for boxes
                    enemy_classes = ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']

                    for d in detections:
                        class_name = d.get('class_name', 'unknown')
                        x_center = d.get('x_center', 0.5)
                        y_center = d.get('y_center', 0.5)

                        # Convert normalized coords to pixel coords
                        img_w, img_h = img.size
                        cx = int(x_center * img_w)
                        cy = int(y_center * img_h)

                        # Approximate box size (since we don't have w/h)
                        # Scale box size based on image dimensions
                        box_half = max(40, int(min(img_w, img_h) * 0.04))  # ~4% of image size

                        # Choose color
                        if class_name in enemy_classes:
                            color = (255, 0, 0)  # Red for enemies
                            label = f"ENEMY: {class_name}"
                        elif class_name == 'BonusBox':
                            color = (255, 255, 0)  # Yellow for boxes
                            label = "BOX"
                        else:
                            color = (0, 255, 0)  # Green for other
                            label = class_name

                        # Draw box
                        draw.rectangle(
                            [cx - box_half, cy - box_half, cx + box_half, cy + box_half],
                            outline=color,
                            width=3
                        )
                        # Draw label
                        draw.text((cx - box_half, cy - box_half - 15), label, fill=color)

                except Exception:
                    pass  # Don't fail if drawing fails

            # Higher resolution for VLM to see enemies clearly
            # Maintain aspect ratio to avoid distortion (game is usually 16:9)
            img = self._resize_maintain_aspect(img, 1280, 720)

            buffer = BytesIO()
            img.save(buffer, format='JPEG', quality=90)  # Higher quality too
            return base64.b64encode(buffer.getvalue()).decode('utf-8')
        except Exception:
            return None

    def _save_screenshot(self, frame: np.ndarray, path: Path):
        """Save screenshot to disk."""
        try:
            if frame.dtype != np.uint8:
                frame = (frame * 255).astype(np.uint8)
            # Convert BGR to RGB
            import cv2
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            # Maintain aspect ratio to avoid distortion
            img = self._resize_maintain_aspect(img, 1280, 720)
            img.save(path, quality=90)
        except Exception:
            pass

    def _save_annotated_screenshot(self, frame: np.ndarray, detections: list, path: Path):
        """
        Save screenshot WITH bounding boxes drawn - exactly what VLM sees.
        This is crucial for debugging VLM responses.
        """
        try:
            if frame.dtype != np.uint8:
                frame = (frame * 255).astype(np.uint8)

            # CRITICAL: Convert BGR (OpenCV) to RGB (PIL) - fixes green tint!
            import cv2
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)

            # Draw detection boxes (same as _encode_image)
            if detections:
                from PIL import ImageDraw
                draw = ImageDraw.Draw(img)
                enemy_classes = ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']

                for d in detections:
                    class_name = d.get('class_name', 'unknown')
                    x_center = d.get('x_center', 0.5)
                    y_center = d.get('y_center', 0.5)

                    img_w, img_h = img.size
                    cx = int(x_center * img_w)
                    cy = int(y_center * img_h)
                    # Scale box size based on image dimensions
                    box_half = max(40, int(min(img_w, img_h) * 0.04))

                    if class_name in enemy_classes:
                        color = (255, 0, 0)
                        label = f"ENEMY: {class_name}"
                    elif class_name == 'BonusBox':
                        color = (255, 255, 0)
                        label = "BOX"
                    else:
                        color = (0, 255, 0)
                        label = class_name

                    draw.rectangle(
                        [cx - box_half, cy - box_half, cx + box_half, cy + box_half],
                        outline=color, width=4  # Thicker lines
                    )
                    draw.text((cx - box_half, cy - box_half - 18), label, fill=color)

            # Higher resolution for debugging - maintain aspect ratio
            img = self._resize_maintain_aspect(img, 1280, 720)
            img.save(path, quality=92)
        except Exception as e:
            print(f"[SelfImprover] Error saving annotated screenshot: {e}")

    def _save_critique_debug(self, critique_id: int, prompt: str, detections: list,
                             bot_action: Dict, context: Dict, vlm_result: Dict):
        """
        Save full debug data for each critique - allows reviewing what was sent to VLM.

        Creates a JSON file with:
        - The full prompt sent to VLM
        - Detection data (what YOLO claimed)
        - Bot action at that moment
        - VLM's response
        - Timestamp
        """
        try:
            debug_dir = self.session_dir / "debug"
            debug_dir.mkdir(exist_ok=True)

            debug_data = {
                'critique_id': critique_id,
                'timestamp': time.time(),
                'timestamp_human': time.strftime('%Y-%m-%d %H:%M:%S'),

                # What was sent to VLM
                'prompt': prompt,

                # Detection data
                'detections': detections,
                'num_enemies_claimed': len([d for d in detections if d.get('class_name') in
                                           ['Devo', 'Lordakia', 'Mordon', 'Saimon', 'Sibelon', 'Struener']]),
                'num_boxes_claimed': len([d for d in detections if d.get('class_name') == 'BonusBox']),

                # Bot state
                'bot_action': bot_action,
                'mode': context.get('mode', 'PASSIVE'),
                'idle_time': context.get('idle_time', 0),

                # Keyboard action state (critical for attack detection)
                'keyboard_actions': {
                    'ctrl_attack': bot_action.get('ctrl_attack', False),
                    'space_rocket': bot_action.get('space_rocket', False),
                    'shift_special': bot_action.get('shift_special', False),
                },
                'is_attacking': context.get('is_attacking', False),

                # VLM response
                'vlm_result': vlm_result,
                'vlm_quality': vlm_result.get('bot_quality', 'unknown'),
                'vlm_reasoning': vlm_result.get('reasoning', ''),
                'vlm_actually_visible': vlm_result.get('actually_visible', {}),

                # Screenshot reference
                'screenshot': f"critique_{critique_id:04d}.jpg"
            }

            debug_path = debug_dir / f"critique_{critique_id:04d}.json"
            with open(debug_path, 'w') as f:
                json.dump(debug_data, f, indent=2, default=str)

        except Exception as e:
            print(f"[SelfImprover] Error saving debug data: {e}")

    def _query_vlm(self, image_b64: str, prompt: str) -> Optional[Dict]:
        """
        Query LM Studio VLM with game knowledge system prompt.

        System prompt is loaded from: darkorbit_bot/data/vlm_system_prompt.txt
        Edit that file to customize VLM behavior!
        """
        try:
            url = f"{self.base_url}/v1/chat/completions"
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": self.get_system_prompt()  # Loaded from file!
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}}
                        ]
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }

            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()

            result = response.json()
            raw_text = result['choices'][0]['message']['content']

            # Parse JSON
            text = raw_text.strip()
            if '```' in text:
                start = text.find('{')
                end = text.rfind('}') + 1
                text = text[start:end]

            return json.loads(text)

        except requests.exceptions.ConnectionError:
            print("[SelfImprover] LM Studio not running")
            return None
        except json.JSONDecodeError:
            return None
        except Exception as e:
            print(f"[SelfImprover] VLM error: {e}")
            return None

    def _log_correction(self, correction: Correction, sequence_stats: Dict = None):
        """Log a correction to console."""
        quality_icon = {
            'good': '✅',
            'needs_improvement': '⚠️',
            'bad': '❌'
        }.get(correction.quality.lower(), '❓')

        print(f"\n[VLM Critique] {quality_icon} {correction.quality.upper()}")
        print(f"   Situation: {correction.situation} | Threat: {correction.threat_level}")

        # Show sequence analysis stats if available
        if sequence_stats:
            enemies = sequence_stats.get('real_enemies_seen', 0)
            boxes = sequence_stats.get('real_boxes_seen', 0)
            false_pos = sequence_stats.get('false_positive_detections', 0)
            missed = sequence_stats.get('opportunities_missed', 0)
            print(f"   Sequence: enemies={enemies}, boxes={boxes}, false_pos={false_pos}, missed_opps={missed}")

        print(f"   Bot did: move to ({correction.bot_action['move_x']:.2f}, {correction.bot_action['move_y']:.2f}), click={correction.bot_action['clicked']}")
        print(f"   Should: move to ({correction.correct_action['move_x']:.2f}, {correction.correct_action['move_y']:.2f}), click={correction.correct_action['should_click']}")
        target_info = correction.correct_action.get('target_name') or correction.correct_action.get('priority', 'N/A')
        print(f"   Target: {correction.correct_action['target_type']} ({target_info})")
        print(f"   Reason: {correction.reasoning[:100]}...")

    def _save_corrections(self):
        """Save all corrections to JSON."""
        if not self.session_dir or not self.corrections:
            return

        filepath = self.session_dir / "corrections.json"
        data = {
            'session': self.session_dir.name,
            'total_critiques': self.total_critiques,
            'good': self.good_count,
            'needs_improvement': self.needs_improvement_count,
            'bad': self.bad_count,
            'corrections': [asdict(c) for c in self.corrections]
        }

        # Custom encoder to handle numpy types
        def json_serializer(obj):
            if isinstance(obj, (np.bool_, np.integer)):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2, default=json_serializer)

        print(f"\n[SelfImprover] Saved {len(self.corrections)} corrections to {filepath}")

    def _print_stats(self):
        """Print session statistics."""
        print("\n" + "="*50)
        print("  SELF-IMPROVEMENT SESSION STATS")
        print("="*50)
        print(f"   Total critiques: {self.total_critiques}")
        print(f"   Good actions: {self.good_count} ({100*self.good_count/max(self.total_critiques,1):.0f}%)")
        print(f"   Needs improvement: {self.needs_improvement_count} ({100*self.needs_improvement_count/max(self.total_critiques,1):.0f}%)")
        print(f"   Bad actions: {self.bad_count} ({100*self.bad_count/max(self.total_critiques,1):.0f}%)")
        print("="*50)


def load_corrections_for_training(corrections_dir: str = "data/corrections") -> List[Dict]:
    """
    Load corrections and convert to training samples.

    The key: we use the CORRECT action (from VLM) as the target,
    not the bot's action. This teaches the network what it SHOULD do.

    Returns:
        List of training samples with state_vector and correct_action
    """
    corrections_path = Path(corrections_dir)
    samples = []

    for session_dir in corrections_path.glob("session_*"):
        corrections_file = session_dir / "corrections.json"
        if not corrections_file.exists():
            continue

        with open(corrections_file, 'r') as f:
            data = json.load(f)

        for c in data.get('corrections', []):
            # Only use corrections where VLM provided a specific action
            correct_action = c.get('correct_action', {})
            state_vector = c.get('state_vector')

            if not state_vector or not correct_action.get('move_x'):
                continue

            # Weight by quality - bad actions = high weight (need to learn most)
            quality = c.get('quality', 'unknown')
            if quality == 'bad':
                weight = 2.0  # High weight - definitely wrong
            elif quality == 'needs_improvement':
                weight = 1.5  # Medium weight
            else:
                weight = 0.5  # Low weight - already good

            samples.append({
                'state': state_vector,
                'action': [
                    correct_action.get('move_x', 0.5),
                    correct_action.get('move_y', 0.5),
                    1.0 if correct_action.get('should_click', False) else 0.0,
                    0.0,  # key_action
                    0.0   # wait_time
                ],
                'mode': c.get('bot_action', {}).get('mode', 'PASSIVE'),
                'weight': weight,
                'source': 'correction'
            })

    print(f"[Corrections] Loaded {len(samples)} correction samples for training")
    return samples


def main():
    """Demo/standalone mode."""
    import argparse

    parser = argparse.ArgumentParser(description='Self-improvement system')
    parser.add_argument('--model', type=str, default='qwen/qwen3-vl-8b')
    parser.add_argument('--url', type=str, default='http://localhost:1234')
    parser.add_argument('--interval', type=float, default=3.0,
                       help='Seconds between critiques')

    args = parser.parse_args()

    print("\n" + "="*60)
    print("  SELF-IMPROVEMENT SYSTEM")
    print("="*60)
    print("\nThis system watches the bot play and generates corrections.")
    print("Corrections can be used to improve training.")
    print("\nTo use with bot_controller.py, add this integration:")
    print("  from reasoning.self_improver import SelfImprover")
    print("  improver = SelfImprover()")
    print("  improver.start_session()")
    print("  improver.start_watching()")
    print("  # In control loop: improver.submit_frame(frame, context)")
    print("="*60)

    # Check VLM connection
    if requests:
        try:
            resp = requests.get(f"{args.url}/v1/models", timeout=5)
            if resp.status_code == 200:
                models = resp.json().get('data', [])
                print(f"\nLM Studio connected! {len(models)} model(s)")
        except Exception:
            print("\nWarning: LM Studio not available")


if __name__ == "__main__":
    main()
